+++
title = "The AI boredom"
date = 2025-01-07T20:00:00+02:00
draft = true
categories = ["ai", "thoughts"]
tags = ["ai"]
description = "Unreliability at its best"
disqus = false
+++


## Summary by the author...
...to not waste energy (and money) with summarization algorithms.  
"
I don't share the same enthusiasm as the Tech World about the current generation of Artificial Intelligence (AI) 
products because:
1. Most of them do not bring value to the people (think outside the "tech enthousiasts" world),
2. They are unreliable,
3. They profit mostly to big - unreliable - companies,
4. They cost too much energy,
5. The responsibility of AI actions is still questionable.
"

## Now, the content

I am sorry but I don't share the enthusiasm of the Tech World about latest Artificial Intelligence (AI) products.
Actually, I feel bored.

Since more a decade now (at least), Artifical Intelligence achieved great progress, from scientific grounbreaking 
(AlphaGo, AlphaFold, YOLO, Live Segmentation of videos, ...) to... ugly content generation for multi-billions companies.

{{< youtube id="4RSTupbfGog" autoplay="false" >}}

Unfortunately, nowadays, AI becames a shortword for "LLM" in the Tech World and, as a former AI Engineer who was formed on 
(what we call now) statistical machine learning and the true value of explainable methods, this little thing irritates 
me at the most highest level as possible.

AI was made for great promises to solve many of our actual problems in society:
* help everyday people in boring tasks to focus on something valuable,
* help in detecting deceases and creating new drugs to help people with new or rare deceases,
* help to solve internal crisis in our society like "unbiased algorithms for society's final decision",
* help to detect harmful content on the web, and report or block them in real-time,
* help to detect cyber-attacks, and report and block them in real-time,
* help people in distress,
* etc.  
The common word here is "help" and, aside the first item for some people and maybe the second item in the future, 
it failed in **every** other task.

### There is no goal

Instead of all of this, we have emoji-generator models on our phones, trained on very big power-unefficient datacenters,
 and consuming our local battery and memory.
Tech companies obviously don't know where to go with AI, and even if they don't need it they want it for investment 
reasons, like crypto and NFT before the AI-Hype era.
To demonstrate this, the best example comes from one the most recent and insignificant Apple ad:

{{< youtube id="A0BXZhdDqZM" >}}

### The unreliability problems

We might ask why new AI models are not really used (yet) for critical problems, exposed previously.
I have the feeling this is because of their many **unreliability** problems.  
There is doubt that there is reliability problems are common in recent AI models, as they can't be deeply tested in achieving a complete task - compared to an expert -, and as they are being trained on biased and unrealiable data.
Also, I think that AI models will continue to be unreliable if they are continuously treated as black boxes, and if AI models are still developed by major unreliable tech companies like Microsoft, Meta, Google, or OpenAI.  
To demonstrate this idea, let's take an example: if new AI products are so reliable, powerful and unbiased today, why they are not used to detect and block, in real-time, hateful content?
But no, instead, Meta think that the best solution - for this **very serious problem** - is [to rely on... humans](https://futurism.com/zuckerberg-meta-announcement-hate-speech).

### But who is responsible for AI actions?

The responsibility of the actions of an AI is also to be considered very seriously, and most companies do not share this idea.
There are many many examples of this on the internet since a few years, but two are really to put in front for this example:
1. the [aircraft company Air Canada and its responsability for its (funny) chatbot actions](https://www.washingtonpost.com/travel/2024/02/18/air-canada-airline-chatbot-ruling/),
2. and, in another measure, the [responsability of Character.AI about the death of a teenager](https://www.nbcnews.com/tech/characterai-lawsuit-florida-teen-death-rcna176791).  
With responsability comes morale actions, and it seems Tech Companies (outside EU maybe) do not care about morality 
as soon as they do not go in courts, or get bad buzz.

### The energy crisis

[The energy crisis is also a problem with those kinds of insignificant (or toys) AI](https://spectrum.ieee.org/ai-energy-consumption), 
but I guess thinking about World impacts is way too much to ask for big companies CEOs and shareholders.
Instead, CEOs are too much busy to discuss about how to replace experts by unreliable algorithms.

### The wrong leaders

Talking about CEOs, let's talk about "AI figures"...  
I have no double those people are smart, but I don't think we should consider them as serious in depicting the future, 
and I don't think we should share their vision too.
Some CEOs like Sam Altman think the best about Artificial Intelligence [is to replace normal people](https://futurism.com/sam-altman-replace-normal-people-ai) in their jobs with unreliable algorithms.
In my head, you would think of "replacing <A> to <B>" is <B> is:
* better than <A> for doing a given task, at any cost,
* or <B> equals to <A> in terms of quality and delivery **but** cheaper.  
But we trully don't achieve this kind of level yet, so **why** express this kind of really dumb idea, and why this person
has so much credits in the tech world instead of being ignored?  
Also, is it possible to know how people will spend 30$ in monthly AI-subscriptions if those people lose their job?

### Are we fucked ?

So, all of this is what we call "The Future"? Is this that fucked?  
To be honest, I thought that Artificial Intelligence would remove all the unpleasant tasks I have to do everyday instead
of focusing my attention to pleasant things, but this still not the case... and my automatic vacuum cleaner is still
as dumb as it was five years ago.  

The only exception I see, despite all of what I just said, comes from a London company, owned by Google, called 
[DeepMind](https://deepmind.google).
DeepMind raised the bar of great scientific breakthrough in the World since more than a decade now, and seems to really try
 to help people in every way.
I encourage you to take a look at [their breakthroughs portfolio](https://deepmind.google/research/breakthroughs/) to 
acknowledge their scientific achievements.

This is all the reasons why, I am sorry, but I do not share your enthousiasm about the latest AI-based emoji-generator.
